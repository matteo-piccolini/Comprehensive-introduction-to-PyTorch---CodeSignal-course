{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68eca7a2-9a0d-4cb9-b4a9-70e711cab1f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Introduction to PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64201e0b-fe3b-4309-a944-89c21d75f6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fecc0bb-fa19-4a9c-a966-a427f2f05667",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Tensors: creation and basic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ed91b99-a5f0-4740-9bdd-ea7499d597d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "\n",
      "Shape of tensor: torch.Size([2, 3])\n",
      "Data type of tensor: torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor: method tensor() of torch\n",
    "tensor_example = torch.tensor([[1,2,3], [4,5,6]])\n",
    "print(tensor_example)\n",
    "\n",
    "# Displaying basic properties of tensors\n",
    "print()\n",
    "print(f\"Shape of tensor: {tensor_example.shape}\")\n",
    "print(f\"Data type of tensor: {tensor_example.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc52a53a-8aef-47de-b777-bd9a6de58864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Addition:\n",
      "tensor([[ 6,  8],\n",
      "        [10, 12]], dtype=torch.int32)\n",
      "\n",
      "Tensor Addition:\n",
      "tensor([[ 6,  8],\n",
      "        [10, 12]], dtype=torch.int32)\n",
      "-------------------------------------------------------------\n",
      "Element-wise Multiplication:\n",
      "tensor([[ 5, 12],\n",
      "        [21, 32]], dtype=torch.int32)\n",
      "\n",
      "Element-wise Multiplication:\n",
      "tensor([[ 5, 12],\n",
      "        [21, 32]], dtype=torch.int32)\n",
      "-------------------------------------------------------------\n",
      "Matrix Multiplication:\n",
      "tensor([[ 5],\n",
      "        [11]], dtype=torch.int32)\n",
      "-------------------------------------------------------------\n",
      "Broadcasted Addition (Adding scalar value):\n",
      "tensor([[6, 7],\n",
      "        [8, 9]], dtype=torch.int32)\n",
      "\n",
      "Broadcasted Addition:\n",
      "tensor([[2, 3],\n",
      "        [5, 6]], dtype=torch.int32)\n",
      "\n",
      "Broadcasted Multiplication:\n",
      "tensor([[1, 2],\n",
      "        [6, 8]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Creating two tensors\n",
    "tensor_a = torch.tensor([[1, 2], [3, 4]], dtype = torch.int32)\n",
    "tensor_b = torch.tensor([[5, 6], [7, 8]], dtype = torch.int32)\n",
    "\n",
    "# Tensor element-wise addition\n",
    "tensor_sum = torch.add(tensor_a, tensor_b)\n",
    "print(f\"Tensor Addition:\\n{tensor_sum}\\n\")\n",
    "# Or equivalently\n",
    "tensor_sum = tensor_a + tensor_b\n",
    "print(f\"Tensor Addition:\\n{tensor_sum}\")\n",
    "\n",
    "print(\"-------------------------------------------------------------\")\n",
    "\n",
    "# Element-wise Multiplication\n",
    "tensor_product = torch.mul(tensor_a, tensor_b)\n",
    "print(f\"Element-wise Multiplication:\\n{tensor_product}\\n\")\n",
    "# Or equivalently\n",
    "tensor_product = tensor_a * tensor_b\n",
    "print(f\"Element-wise Multiplication:\\n{tensor_product}\")\n",
    "\n",
    "print(\"-------------------------------------------------------------\")\n",
    "\n",
    "# Matrix Multiplication\n",
    "tensor_c = torch.tensor([[1], [2]], dtype = torch.int32) # 2x1 tensor\n",
    "tensor_matmul = torch.matmul(tensor_a, tensor_c)\n",
    "print(f\"Matrix Multiplication:\\n{tensor_matmul}\")\n",
    "\n",
    "print(\"-------------------------------------------------------------\")\n",
    "\n",
    "# Broadcasted Addition (Tensor + scalar)\n",
    "tensor_add_scalar = tensor_a + 5\n",
    "print(f\"Broadcasted Addition (Adding scalar value):\\n{tensor_add_scalar}\\n\")\n",
    "\n",
    "# Broadcasted Addition between tensors of different shapes (same as torch.add)\n",
    "broadcasted_sum = tensor_a + tensor_c\n",
    "print(f\"Broadcasted Addition:\\n{broadcasted_sum}\\n\")\n",
    "\n",
    "# Broadcasted Multiplication between tensors of different shapes (same as torch.mul)\n",
    "broadcasted_mul = tensor_a * tensor_c\n",
    "print(f\"Broadcasted Multiplication:\\n{broadcasted_mul}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09ee7c67-35ce-4f15-a11b-e50679314856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "\n",
      "Reshaped Tensor:\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "\n",
      "Flattened Tensor:\n",
      "tensor([1, 2, 3, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "# Creating a tensor for manipulation\n",
    "tensor_a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(f\"Original Tensor:\\n{tensor_a}\\n\")\n",
    "\n",
    "# Reshape the tensor: method view() of the tensor object\n",
    "reshaped_tensor = tensor_a.view(3, 2)\n",
    "print(f\"Reshaped Tensor:\\n{reshaped_tensor}\\n\")\n",
    "\n",
    "# Flatten the tensor\n",
    "flattened_tensor = tensor_a.view(-1)\n",
    "print(f\"Flattened Tensor:\\n{flattened_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a086fb2-1706-4a46-9972-f02375076f39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### TensorDatasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "711d090c-3258-42db-92fc-2bb87eb7baf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[0]: tensor([1., 2.]), y[0]: 0\n",
      "X[1]: tensor([2., 1.]), y[1]: 1\n",
      "X[2]: tensor([3., 4.]), y[2]: 0\n",
      "X[3]: tensor([4., 3.]), y[3]: 1\n"
     ]
    }
   ],
   "source": [
    "# Define a simple array of input data\n",
    "X = np.array([[1.0, 2.0], [2.0, 1.0], [3.0, 4.0], [4.0, 3.0]])  # 4x2\n",
    "\n",
    "# Define the target outputs for our dataset\n",
    "y = np.array([0, 1, 0, 1])  # 1x4\n",
    "\n",
    "# Convert X and y into PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype = torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype = torch.int32)\n",
    "\n",
    "# Create a tensor dataset \n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Print x and y of the TensorDataset\n",
    "for i in range(len(dataset)):\n",
    "    X_sample, y_sample = dataset[i]\n",
    "    print(f\"X[{i}]: {X_sample}, y[{i}]: {y_sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fcf156f-e243-44e0-9167-9733cd3578e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch X:\n",
      "tensor([[1., 2.],\n",
      "        [2., 1.]])\n",
      "Batch y:\n",
      "tensor([0, 1], dtype=torch.int32)\n",
      "\n",
      "Batch X:\n",
      "tensor([[3., 4.],\n",
      "        [4., 3.]])\n",
      "Batch y:\n",
      "tensor([0, 1], dtype=torch.int32)\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Batch X:\n",
      "tensor([[1., 2.],\n",
      "        [2., 1.]])\n",
      "Batch y:\n",
      "tensor([0, 1], dtype=torch.int32)\n",
      "\n",
      "Batch X:\n",
      "tensor([[4., 3.],\n",
      "        [3., 4.]])\n",
      "Batch y:\n",
      "tensor([1, 0], dtype=torch.int32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a data loader\n",
    "dataloader = DataLoader(dataset, batch_size = 2, shuffle = False)\n",
    "\n",
    "# Iterate through the dataloader\n",
    "for batch_X, batch_y in dataloader:\n",
    "    print(f\"Batch X:\\n{batch_X}\")\n",
    "    print(f\"Batch y:\\n{batch_y}\\n\")\n",
    "\n",
    "print(\"-------------------------------------------------------------\")\n",
    "\n",
    "# Create a shuffling data loader\n",
    "dataloader_shuffle = DataLoader(dataset, batch_size = 2, shuffle = True)\n",
    "\n",
    "# Iterate through the dataloader\n",
    "for batch_X, batch_y in dataloader_shuffle:\n",
    "    print(f\"Batch X:\\n{batch_X}\")\n",
    "    print(f\"Batch y:\\n{batch_y}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf4065e-77b3-48b2-afb5-506af377b686",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Processing Tensors with PyTorch Neural Network Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "737a5654-636a-4875-a310-b5db92b80c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor:\n",
      "tensor([[1., 2.]])\n",
      "\n",
      "Output Tensor Before Activation:\n",
      "tensor([[-0.0475,  2.4381,  0.7913]], grad_fn=<AddmmBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define an input tensor with specific values\n",
    "input_tensor = torch.tensor([[1.0, 2.0]], dtype=torch.float32)  # 1x2\n",
    "\n",
    "# Create a linear layer with 2 input features and 3 output features\n",
    "layer = nn.Linear(in_features = 2, out_features = 3)\n",
    "\n",
    "# Process the input through the linear layer to get initial output\n",
    "# A linear layer operates via the formula: y = W*x + b, with W weigth matrix, x input vector, y output, b bias vector\n",
    "# Weights and biases are initialized randomly\n",
    "output_tensor = layer(input_tensor)\n",
    "\n",
    "# Display the original input tensor\n",
    "print(f\"Input Tensor:\\n{input_tensor}\\n\")\n",
    "\n",
    "# Display the output before activation to see the linear transformation effect\n",
    "print(f\"Output Tensor Before Activation:\\n{output_tensor}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44d9bb3-59e1-4f17-aa0e-43e0d9a65802",
   "metadata": {},
   "source": [
    "_AddmmBackward0_ in the output means that PyTorch is keeping track of this operation, which will help compute the gradients automatically during model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261ffe81-c53c-4714-b801-3d12ce50ffc2",
   "metadata": {},
   "source": [
    "Activation functions introduce non-linearity into the model, enabling it to handle more complex patterns in the data.\n",
    "Two commonly used activation functions are ReLU (Rectified Linear Unit) and Sigmoid.\n",
    "\n",
    "Effect of the ReLU function: it zeroes out any negative values, converting them to zero, while keeping positive values unchanged. This introduces non-linearity into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd7579b8-48f5-4500-918e-91f2e220d15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Tensor After ReLU Activation:\n",
      "tensor([[0.0000, 2.4381, 0.7913]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define a ReLU activation function to introduce non-linearity\n",
    "relu = nn.ReLU()\n",
    "\n",
    "# Apply the ReLU function to the output of the linear layer\n",
    "activated_output_relu = relu(output_tensor)\n",
    "\n",
    "# Display the output after activation to observe the effect of ReLU\n",
    "print(f\"Output Tensor After ReLU Activation:\\n{activated_output_relu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ec6cc8-dcb3-43c9-8f30-7ae3dcb89015",
   "metadata": {},
   "source": [
    "The _grad_fn=ReluBackward0_ shows that the ReLU operation is also being tracked for automatic differentiation during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fb1c84-89d4-4cf9-89d8-7af14eefa16e",
   "metadata": {},
   "source": [
    "Sigmoid squashes values between 0 and 1: it is useful for binary classification and probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0845972d-891b-4378-941e-27852a8ed3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Tensor After Sigmoid Activation:\n",
      "tensor([[0.4881, 0.9197, 0.6881]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define a Sigmoid activation function\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "# Apply the Sigmoid function to the output of the linear layer\n",
    "activated_output_sigmoid = sigmoid(output_tensor)\n",
    "\n",
    "# Display the output after applying the Sigmoid function\n",
    "print(f\"Output Tensor After Sigmoid Activation:\\n{activated_output_sigmoid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6fcb22-872d-4ace-8075-0a307b9f94bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Building a Neural Network in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da0ffa5d-4ce5-4ce3-9e51-afe317f9136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837412ff-0eef-419e-95cb-0e382c1fabf2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Initializing a Neural Network model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cde5e95-d16b-4d35-ad39-ed90712672de",
   "metadata": {},
   "source": [
    "The building blocks of a Neural Network model are _modules_. PyTorch’s modules are encapsulated as Python classes, with the base one being the nn.Module class. Any model created in PyTorch is a subclass of the nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12cfd2ad-234c-43e3-b0fe-32de38be0298",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):   # start by defining a class named SimpleNN which inherits from PyTorch's nn.Module\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()   # retrieve the parent class of SimpleNN, i.e., nn.Module(), and apply its initialization method to the class which is being built, labelled \"self\" \n",
    "        self.layer1 = nn.Linear(in_features = 2, out_features = 10)   # first fully connected layer: input size 2, output size 10\n",
    "        self.relu = nn.ReLU()    # ReLU activation function to be used after layer1\n",
    "        self.layer2 = nn.Linear(in_features = 10, out_features = 1)   # Second fully connected layer: the input size of each layer must always match the output size of the preceding one\n",
    "        self.sigmoid = nn.Sigmoid()   # Sigmoid activation function to be used after layer2\n",
    "\n",
    "    def forward(self, x):   # this method orchestrates the flow of data, specifying the sequence in which the layers and activation functions are applied to the input\n",
    "        x = self.layer1(x)   # apply layer1 (input: 2, output: 10)\n",
    "        x = self.relu(x)   # apply ReLU activation function (output: 10)\n",
    "        x = self.layer2(x)   # apply layer2 (input: 10, output: 1)\n",
    "        x = self.sigmoid(x)   # apply Sigmoid activation function (output: 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c435fa8-ed9c-4036-97e0-4ed18968801d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (layer1): Linear(in_features=2, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (layer2): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SimpleNN()   # create an instance of the SimpleNN class, representing the model to train\n",
    "print(model)   # print the architecture of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c3d1ca-0fb0-4f5c-8c0e-848f565beb51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Initializing a Neural Network model using PyTorch's nn.Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbac7b5c-8029-4701-ac09-175024f975ef",
   "metadata": {},
   "source": [
    "The Sequential model is a linear stack of layers, each having one input tensor and one output tensor, that you can easily create using the nn.Sequential() function. It makes the building of models more comfortable, and the created models are shorter and more readable.\n",
    "\n",
    "The previously discussed method to build neural networks, consisting in definining a custom class, adding layers inside the constructor, and implementing the forward method, is flexible but can be an overkill for simple models. Here is where Sequential Models are handy — creating compact and understandable models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c00f5ba-cc43-4d17-8062-74fcc7415d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Model Architecture:\n",
      " Sequential(\n",
      "  (0): Linear(in_features=2, out_features=10, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (3): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 10),  # First layer: input size 2, output size 10\n",
    "    nn.ReLU(),         # ReLU activation function\n",
    "    nn.Linear(10, 1),  # Second layer: input size 10, output size 1\n",
    "    nn.Sigmoid()       # Sigmoid activation function\n",
    ")\n",
    "\n",
    "print(\"Sequential Model Architecture:\\n\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ba62eb-00b2-4f07-a440-736e2de1c969",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Training a Neural Network Model with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0baa78-1382-4f7b-b414-a5bd5fd1edf3",
   "metadata": {},
   "source": [
    "Training a model is a process of learning the weight parameters that minimize the error on the training data. The process involves passing data through the model (forward propagation), computing the loss (how far the model's prediction is from the actual value), and then adjusting the weights using this loss (Backward Propagation).\n",
    "\n",
    "To do this in PyTorch, we will need our training data, a defined model, a loss function, and an optimizer for adjusting the weights.\n",
    "\n",
    "Supervised learning techniques require input data (features) and output data (target/labels). In the following example, the input represents the average goals scored by a soccer team and the average goals conceded by their opponent during the season (each row represents a team). The output is binary, indicating whether the team is likely to win a match against this specific opponent (1) or not (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fd75639-3928-4203-b0c2-80e9e64770d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Input features [Average Goals Scored, Average Goals Conceded by Opponent]\n",
    "X = torch.tensor([\n",
    "    [3.0, 0.5], [1.0, 1.0], [0.5, 2.0], [2.0, 1.5],\n",
    "    [3.5, 3.0], [2.0, 2.5], [1.5, 1.0], [0.5, 0.5],\n",
    "    [2.5, 0.8], [2.1, 2.0], [1.2, 0.5], [0.7, 1.5]\n",
    "], dtype=torch.float32)   # 12x2 - twelve teams considered, two features per datapoint (features = num of columns!)\n",
    "\n",
    "# Target outputs [1 if the team is likely to win, 0 otherwise]\n",
    "y = torch.tensor([[1], [0], [0], [1], [1], [0], [1], [0], [1], [0], [1], [0]], dtype=torch.float32)   # 12x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934f8be4-77b8-47bd-9b4a-6f30eedeef8c",
   "metadata": {},
   "source": [
    "It is important to note that we've used dtype=torch.float32 for both X and y as our loss function (Binary Cross-Entropy) requires the target tensor y to be in floating-point format. Other loss functions may require different data types, so it's crucial to ensure compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6c674ab-6d2e-44bb-8844-47bd74189554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b319e8d-5df1-4e80-8654-ec2a4f81a386",
   "metadata": {},
   "source": [
    "To train the NN model we need _criterion_ (also known as a _loss function_) and an _optimizer_.\n",
    "\n",
    "The criterion measures how far the model's predictions are from the actual output. PyTorch provides several loss function classes, and for this binary classification task we use the Binary Cross-Entropy (BCE) Loss.\n",
    "\n",
    "The optimizer is used to update the model parameters (weights and biases) based on the derivatives computed during backpropagation. PyTorch offers several optimization algorithms under torch.optim. In this example, we use Adam with a learning rate of 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "479fe866-b9f3-4dde-b0b5-11ab2d54c044",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)   # Learning rate is very important!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be42fd20-717a-4872-85e9-7285a48916b2",
   "metadata": {},
   "source": [
    "Training a neural network typically involves iteratively passing data through our model, calculating the loss, and backpropagating the loss to update our model. This process is repeated for a certain number of epochs. An epoch is one complete pass through the entire training dataset.\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "**1. Model Training Mode:** model.train() places the model in training mode, enabling necessary features that should only be \n",
    "active during training. It's good practice to keep it inside the loop to ensure the model is consistently set to training mode at the start of each epoch, especially if you might switch to other modes for some reason during the training process.\n",
    "\n",
    "**2. Reset Gradients:** optimizer.zero_grad() is called to reset the gradients of the model parameters; this is crucial because gradients accumulate by default in PyTorch.\n",
    "\n",
    "**3. Forward Pass:** outputs = model(X) computes the model's predictions based on the current state of the model parameters.\n",
    "\n",
    "**4. Loss Calculation:** The loss is calculated by comparing the model's predictions (outputs) to the true labels (y) using the pre-defined loss function criterion, which in this case is nn.BCELoss().\n",
    "\n",
    "Following the loss calculation, the backward pass is initiated:\n",
    "\n",
    "**1. Backward Pass:** loss.backward() computes the gradients of the loss with respect to each parameter.\n",
    "\n",
    "**2. Parameter Update:** The optimizer uses these gradients in optimizer.step() to adjust the model's parameters, reducing the loss for the next iteration.\n",
    "\n",
    "This sequence is repeated for a predefined number of epochs (50 in this case), allowing the model to iteratively learn and improve its performance on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b2cf4f9-6208-4a45-baee-635c2a610359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.6675704121589661\n",
      "Epoch 20, Loss: 0.615002453327179\n",
      "Epoch 30, Loss: 0.5430947542190552\n",
      "Epoch 40, Loss: 0.4508834183216095\n",
      "Epoch 50, Loss: 0.36249232292175293\n"
     ]
    }
   ],
   "source": [
    "# Train the model for 50 epochs\n",
    "for epoch in range(50):  \n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    optimizer.zero_grad()  # Zero the gradients for this iteration\n",
    "\n",
    "    outputs = model(X)  # Forward pass: compute predictions\n",
    "\n",
    "    loss = criterion(outputs, y)  # Compute the loss. This is a single-valued tensor!\n",
    "\n",
    "    loss.backward()  # Backward pass: compute the gradient of the loss\n",
    "\n",
    "    optimizer.step()  # Optimize the model parameters based on the gradients\n",
    "\n",
    "    if (epoch+1) % 10 == 0:  # Print every 10 epochs\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")  # Print epoch loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095af034-59e9-42b3-9ceb-6c6bdcf0fa50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Making Predictions with a Trained PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104ef0d4-b3df-4fd2-aee5-2b50b33fc271",
   "metadata": {},
   "source": [
    "We now make predictions using the previously trained model,\n",
    "\n",
    "The first crucial step after training our model is to put it in evaluation mode using _model.eval()_. Indeed, models can behave differently during training and evaluation phases. For example, many components or layers of the model may have certain behaviors that only need to occur during training, like adjusting internal parameters based on the provided data.\n",
    "Putting the model in evaluation mode ensures that these components function correctly for making predictions.\n",
    "\n",
    "When making predictions, we don't need to compute gradients anymore because we're not updating the model's weights. As a good practice, disable gradient calculation to save memory and computation. PyTorch allows to do this by wrapping the prediction code block in with torch.no_grad():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "310f314a-4429-48c0-9cee-2373745b035b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw output: tensor([[0.3834]])\n",
      "Prediction: 0\n"
     ]
    }
   ],
   "source": [
    "# Create a new input tensor\n",
    "new_input = torch.tensor([[4.0, 5.0]], dtype=torch.float32)   #  input data must be of the same type and shape that the model was trained with\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient calculation for inference\n",
    "with torch.no_grad():\n",
    "    # Make a prediction for the new input\n",
    "    prediction = model(new_input)\n",
    "\n",
    "# Print the raw output from the model\n",
    "print(\"Raw output:\", prediction)\n",
    "\n",
    "# Convert the probability to a binary class label\n",
    "print(\"Prediction:\", (prediction > 0.5).int().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f37c856-fba5-4ed0-b216-52c99613593d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Evaluating a Model with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad04d36f-537d-46d7-9c0d-835971e05502",
   "metadata": {},
   "source": [
    "To evaluate a trained model, we make predictions with it and compare them with the actual truth values in a test dataset.\n",
    "\n",
    "The test dataset consists of new data points that the model has never seen before. This helps us understand how well our model generalizes to unseen data.\n",
    "\n",
    "Evaluation metrics interpret the performance of our model. There are many metrics: here we consider _accuracy_.\n",
    "This is a useful measure when the target variable classes in the data are nearly balanced. It is defined as the number of correct predictions made divided by the total number of predictions made. To apply the _accuracy_ evaluation metric, we use **Scikit-Learn**: we import _accuracy_score_ from the _sklearn.metrics_ module.\n",
    "\n",
    "Once the model is trained, we switch it to evaluation mode using model.eval() and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39594efc-6ada-4883-b717-791c2d883d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 1.0, Test loss: 0.2784452438354492\n"
     ]
    }
   ],
   "source": [
    "# Test Features\n",
    "X_test = torch.tensor([[2.5, 1.0], [0.8, 0.8], [1.0, 2.0], [3.0, 2.5]], dtype=torch.float32)   # 4x2\n",
    "# Test Targets\n",
    "y_test = torch.tensor([[1], [0], [0], [1]], dtype=torch.float32)   # 4x1\n",
    "\n",
    "# Import the accuracy_score functionality\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set evaluation mode and disable gradient\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Make Predictions\n",
    "    outputs = model(X_test)   # this is a 4x1 tensor\n",
    "    \n",
    "    # Convert to binary classes\n",
    "    predicted_classes = (outputs > 0.5).int()     # this is a 4x1 tensor\n",
    "    \n",
    "    # Calculate the accuracy on the test data\n",
    "    test_accuracy = accuracy_score(y_test.numpy(), predicted_classes.numpy())\n",
    "\n",
    "    # Calculate the loss on the test data\n",
    "    test_loss = criterion(outputs, y_test)   # this is a single-valued tensor\n",
    "    test_loss = test_loss.item()    # this is a number\n",
    "\n",
    "# Print the test accuracy and loss\n",
    "print(f'\\nTest accuracy: {test_accuracy}, Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daec1e18-1375-4efc-b9d9-b7d36f93cdf9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Modeling the Wine Dataset with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "94cf1640-c054-43fc-b239-33f65285a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b8533e-9e66-4f94-ba2a-f79d5599ea2a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Preprocessing the Wine Dataset for PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b30975-1598-4a8e-be4f-f15ec22178d4",
   "metadata": {},
   "source": [
    "The Wine dataset is a classic dataset in machine learning, often used for classification tasks. It consists of 178 instances of wine where each instance is described by 13 numeric features (such as alcohol content, malic acid, etc.). The target variable is a class label indicating the type of wine (three classes), which will be predicted using the 13 features.\n",
    "\n",
    "* **Step 1.** Load the Wine dataset and explore its structure. We'll use sklearn.datasets to load the dataset.\n",
    "  \n",
    "* **Step 2.** Split the dataset into a training dataset and a test dataset. To do so, we use _train_test_split_ from _sklearn.model_selection_. We use _stratify_ for y to ensure that class proportions are preserved in both training and testing sets. The test_size = 0.3 parameter means that 30% of the data will be used for testing, while the remaining 70% will be used for training.\n",
    "\n",
    "* **Step 3.** Perform feature scaling to ensure that features are on similar scales, which can improve the performance of algorithms. We use _StandardScaler_ from _sklearn.preprocessing_ to transform the features so they all have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "* **Step 4.** To use the data with PyTorch, we need to convert the NumPy arrays into PyTorch tensors. This conversion is essential as PyTorch models require input data in tensor format. We use _float32_ format for features to ensure numeric precision during computations, and _dtype = torch.long_ for labels because classification tasks in PyTorch expect labels to be in integer format. Specifically, _torch.long_ represents 64-bit integer data type, which is required by loss functions such as nn.CrossEntropyLoss in PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff8e6ec7-933e-4144-af71-501d9861ac09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "Target classes: ['class_0' 'class_1' 'class_2']\n",
      "\n",
      "Shape of X_train: (124, 13)\n",
      "Shape of X_test: (54, 13)\n",
      "Shape of y_train: (124,)\n",
      "Shape of y_test: (54,)\n",
      "\n",
      "Unscaled X sample:\n",
      " [1.267e+01 9.800e-01 2.240e+00 1.800e+01 9.900e+01 2.200e+00 1.940e+00\n",
      " 3.000e-01 1.460e+00 2.620e+00 1.230e+00 3.160e+00 4.500e+02]\n",
      "Scaled X sample:\n",
      " [-0.38442565 -1.23482307 -0.49978908 -0.49971329  0.00569959 -0.15638344\n",
      " -0.08598532 -0.50339468 -0.21393306 -1.03577719  1.18406708  0.74723066\n",
      " -0.91541036]\n",
      "\n",
      "Sample of X_train_tensor: tensor([-0.3844, -1.2348, -0.4998, -0.4997,  0.0057, -0.1564, -0.0860, -0.5034,\n",
      "        -0.2139, -1.0358,  1.1841,  0.7472, -0.9154])\n",
      "Sample of y_train_tensor: tensor(1)\n"
     ]
    }
   ],
   "source": [
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "\n",
    "# Explore dataset features and target classes\n",
    "print(\"Features:\", wine.feature_names)\n",
    "print(\"Target classes:\", wine.target_names)\n",
    "\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "\n",
    "# Display the shapes of the resulting splits\n",
    "print(\"\\nShape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "\n",
    "# Initialize the scaler and fit it to the training data\n",
    "scaler = StandardScaler().fit(X_train)   # only fits with training data because test data should not be accessible at this stage\n",
    "\n",
    "# Transform both the training and testing datasets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Display scaled and unscaled samples\n",
    "print(\"\\nUnscaled X sample:\\n\", X_train[0])\n",
    "print(\"Scaled X sample:\\n\", X_train_scaled[0])\n",
    "\n",
    "# Convert scaled data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype = torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype = torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype = torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype = torch.long)\n",
    "\n",
    "# Display example tensors\n",
    "print(\"\\nSample of X_train_tensor:\", X_train_tensor[0])\n",
    "print(\"Sample of y_train_tensor:\", y_train_tensor[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a000825d-190e-45cd-a22a-ea9bba390699",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Building a Multi-Class Classification Model with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec62c8ab-128c-45f7-8bd4-01894d1696f6",
   "metadata": {},
   "source": [
    "* Before we start building our multi-class classification model, we need to load our preprocessed dataset. To maintain modular code, \n",
    "we take the code written in the previous _Preprocessing the Wine Dataset for PyTorch_ section, define a _load_preprocessed_data_ function with it, store it in a macro file _data_preprocessing.py_, and then load it in this file.\n",
    "\n",
    "*  Our machine learning model will be a multi-class feed-forward neural network consisting of linear and non-linear layers, where we have three linear layers and two ReLU (Rectified Linear Unit) activation layers. The output consists of 3 features (the output tensor will have 3 columns): one per possible class.\n",
    "\n",
    "* As a loss function, we will use _Cross-Entropy loss_, which is used when the output needs to be a probability distribution. This loss function expects raw scores (also known as _logits_: one per target class, e.g., 3 per wine in this example) as input and it internally applies the _softmax_ function to convert these scores into probabilities. So, even though _softmax_ is typically used for multi-class classification problems, we don't need to add a separate softmax layer in our model: the loss function handles it for us. Notice that _Cross-Entropy loss_ compares the output, which is a tensor with 3 columns, with y_train or y_test, which is a single-column array.\n",
    "\n",
    "* We use the _Adam_ (Adaptive Moment Estimation) optimizer, which adjusts the model's parameters based on the gradients (which tell us how much the loss would change if we changed the parameters). The _model.parameters()_ call returns an iterator of all the parameters (weights and biases) in our model that need to be optimized. A learning rate of 0.001 is generally a good starting point for many problems.\n",
    "\n",
    "* We then train the model:\n",
    "\n",
    "  * **Step 1.** Set Number of Epochs: Define how many times the model will iterate over the entire training dataset.\n",
    "  * **Step 2.** Initialize History: Create a dictionary to store loss and validation loss values for each epoch.\n",
    "  * **Step 3.** Training Loop: Loop through the training process for the specified number of epochs.\n",
    "  * **Step 4.** Training Phase: Switch to training mode, clear gradients, make predictions, calculate loss, perform backpropagation, update parameters, and store training loss.\n",
    "  * **Step 5.** Evaluation Phase: Switch to evaluation mode, disable gradient calculation, make predictions on validation data (outputs_val), which is not used to train the model but to evaluate how it performs on unseen data, calculate validation loss, and store validation loss.\n",
    "  * **Step 6.** Print Progress: Every 10 epochs, print the current epoch, training loss, and validation loss to monitor progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6807fe52-c73a-400f-a7e4-08d6bc55dac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL CREATION:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=13, out_features=10, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=10, out_features=3, bias=True)\n",
      ")\n",
      "\n",
      "MODEL TRAINING:\n",
      "Epoch [10/150], Loss: 1.0481, Validation Loss: 1.0611\n",
      "Epoch [20/150], Loss: 1.0135, Validation Loss: 1.0317\n",
      "Epoch [30/150], Loss: 0.9749, Validation Loss: 0.9989\n",
      "Epoch [40/150], Loss: 0.9303, Validation Loss: 0.9607\n",
      "Epoch [50/150], Loss: 0.8786, Validation Loss: 0.9168\n",
      "Epoch [60/150], Loss: 0.8214, Validation Loss: 0.8701\n",
      "Epoch [70/150], Loss: 0.7616, Validation Loss: 0.8210\n",
      "Epoch [80/150], Loss: 0.7022, Validation Loss: 0.7721\n",
      "Epoch [90/150], Loss: 0.6458, Validation Loss: 0.7245\n",
      "Epoch [100/150], Loss: 0.5916, Validation Loss: 0.6767\n",
      "Epoch [110/150], Loss: 0.5398, Validation Loss: 0.6295\n",
      "Epoch [120/150], Loss: 0.4899, Validation Loss: 0.5828\n",
      "Epoch [130/150], Loss: 0.4409, Validation Loss: 0.5352\n",
      "Epoch [140/150], Loss: 0.3912, Validation Loss: 0.4854\n",
      "Epoch [150/150], Loss: 0.3406, Validation Loss: 0.4340\n"
     ]
    }
   ],
   "source": [
    "from data_preprocessing import load_preprocessed_data\n",
    "X_train, X_test, y_train, y_test = load_preprocessed_data()\n",
    "\n",
    "# Define the model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(13, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 3)\n",
    ")\n",
    "\n",
    "# Display model's architecture\n",
    "print(f\"MODEL CREATION:\\n{model}\\n\")\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "# Train the model\n",
    "print(\"MODEL TRAINING:\")\n",
    "num_epochs = 150\n",
    "history = {'loss': [], 'val_loss': []}\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)   # shape: 124x3\n",
    "    loss = criterion(outputs, y_train)  \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    history['loss'].append(loss.item())\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs_val = model(X_test)   # shape: 54x3\n",
    "        val_loss = criterion(outputs_val, y_test)\n",
    "        history['val_loss'].append(val_loss.item())\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363f713d-d913-4c12-a022-eb10698bf26a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Deep Model Evaluation with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d199d185-f53e-4bb3-94ef-4444fadb7711",
   "metadata": {},
   "source": [
    "To evaluate our fully trained model, we again calculate the test loss and compute the _accuracy_, which is the fraction of correct predictions over total predictions. We use _torch.no_grad_ to disable gradient calculations, as they are not needed during evaluation, and the _accuracy_score_ function from _sklearn.metrics_ to compute accuracy.\n",
    "\n",
    "We obtain the predicted class labels by selecting the class with the highest value among the 3 characterizing outputs using _torch.max(outputs, 1)_, which returns two tensors: the max value (which we discard using _ as it's not needed) and the index of the max value along dimension 1 (i.e., the index of the class with highest value = highest probability of being the right one), which corresponds to the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db55aed8-8684-4e73-be6d-37d01593a41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9259, Test Loss: 0.4340\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Disables gradient calculation\n",
    "with torch.no_grad():\n",
    "    # Input the test data into the model\n",
    "    outputs = model(X_test)\n",
    "    # Calculate the Cross Entropy Loss\n",
    "    test_loss = criterion(outputs, y_test).item()\n",
    "    # Choose the class with the highest value as the predicted output\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    # Calculate the accuracy\n",
    "    test_accuracy = accuracy_score(y_test, predicted)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a919bb22-76a4-4b3f-b711-da7b00169b47",
   "metadata": {},
   "source": [
    "We now want to visualize the loss data during model evaluation: this is crucial, as it helps in understanding the learning progress of the model over time. We therefore plot the training and validation loss using _matplotlib_, to identify patterns such as overfitting or underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ce4659ed-d03b-4413-949f-91b7a6976cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting actual training and validation loss\n",
    "epochs = range(1, num_epochs + 1)\n",
    "train_loss = history['loss']\n",
    "val_loss = history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs, train_loss, label='Training Loss')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "plt.title('Model Loss During Training')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846a0aaa-2f49-40bd-b5ff-96ac4987b73d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Saving and Loading PyTorch Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901c1ae2-1bea-4f6b-a996-c1a7319a3b81",
   "metadata": {},
   "source": [
    "_torch.save()_ method saves the model into a file ending in .pth, which indicates that this file holds a serialized PyTorch model.\n",
    "\n",
    "Serialization is a process where an object in memory (like our PyTorch model) is converted into a format that can be saved on disk or sent over a network. Hence, when we save a model, we're serializing the entire module.\n",
    "\n",
    "We then load back the model into memory with _torch.load_. Note, that the weights_only parameter is set to False, which means that the entire model is loaded, including the architecture, weights, and other parameters. After loading, it's essential to set the model to evaluation mode with model.eval().\n",
    "\n",
    "Finally, we evaluate the performance of both the original model and the loaded one: the two must be the same to confirm that the saving and loading process preserved the model accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "90d73727-3972-438b-98f7-1baba06d2d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Accuracy: 0.9259\n",
      "Loaded Model Accuracy: 0.9259\n"
     ]
    }
   ],
   "source": [
    "# Save the entire model\n",
    "torch.save(model, 'wine_model.pth')\n",
    "\n",
    "# Load the entire model\n",
    "loaded_model = torch.load('wine_model.pth', weights_only=False)\n",
    "loaded_model.eval()\n",
    "\n",
    "# Verify the loaded model by evaluating it on test data\n",
    "with torch.no_grad():\n",
    "    # Make predictions for both models\n",
    "    model.eval()\n",
    "    original_outputs = model(X_test)\n",
    "    loaded_outputs = loaded_model(X_test)\n",
    "    # Format predictions\n",
    "    _, original_predicted = torch.max(original_outputs, 1)\n",
    "    _, loaded_predicted = torch.max(loaded_outputs, 1)\n",
    "    # Calculate accuracies\n",
    "    original_accuracy = accuracy_score(y_test, original_predicted)\n",
    "    loaded_accuracy = accuracy_score(y_test, loaded_predicted)\n",
    "\n",
    "# Display accuracies for both models\n",
    "print(f'Original Model Accuracy: {original_accuracy:.4f}')\n",
    "print(f'Loaded Model Accuracy: {loaded_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a88381-9d74-4b5b-917e-946bcfbcf60e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# PyTorch Techniques for Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24681d4c-d88a-4472-9a96-6fe308f30356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09d2253-fb32-4c36-8bf7-378f91f2118f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Introduction to Model Checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93afa303-1666-4ffb-bad7-45554ab096aa",
   "metadata": {},
   "source": [
    "Model checkpointing allows to save the state of a model during training, ensuring that the best-performing models are preserved. It involves saving the state of a neural network model at various points during the training process. This is crucial for several reasons:\n",
    "\n",
    "* Prevent Loss of Progress: In case of unexpected interruptions (e.g., power failure, hardware malfunction), checkpointing helps in resuming training from the last saved state.\n",
    "  \n",
    "* Save Best Performing Models: By saving the model whenever it achieves a new best performance on a validation set, we ensure that we retain the best version of our model.\n",
    "\n",
    "Before starting the training loop, we first set up the initial parameters for checkpointing:\n",
    "\n",
    "* _best_loss_ to keep track of the best validation loss. We initialize best_loss to float('inf') to ensure the first validation loss will trigger a model save\n",
    "  \n",
    "* _checkpoint_path_ where the model will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc4d0d90-c2ef-41a7-913e-317425740031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at epoch 0 with validation loss 8.4086!\n",
      "Model saved at epoch 1 with validation loss 7.7307!\n",
      "Model saved at epoch 2 with validation loss 7.0687!\n",
      "Model saved at epoch 3 with validation loss 6.4322!\n",
      "Model saved at epoch 4 with validation loss 5.8552!\n",
      "Model saved at epoch 5 with validation loss 5.4101!\n",
      "Model saved at epoch 6 with validation loss 5.1248!\n",
      "Model saved at epoch 7 with validation loss 4.9540!\n",
      "Model saved at epoch 8 with validation loss 4.8437!\n",
      "Model saved at epoch 9 with validation loss 4.7569!\n",
      "Model saved at epoch 10 with validation loss 4.6734!\n",
      "Model saved at epoch 11 with validation loss 4.5903!\n",
      "Model saved at epoch 12 with validation loss 4.5021!\n",
      "Model saved at epoch 13 with validation loss 4.4082!\n",
      "Model saved at epoch 14 with validation loss 4.3076!\n",
      "Model saved at epoch 15 with validation loss 4.2015!\n",
      "Model saved at epoch 16 with validation loss 4.0908!\n",
      "Model saved at epoch 17 with validation loss 3.9761!\n",
      "Model saved at epoch 18 with validation loss 3.8582!\n",
      "Model saved at epoch 19 with validation loss 3.7377!\n",
      "Model saved at epoch 20 with validation loss 3.6153!\n",
      "Model saved at epoch 21 with validation loss 3.4914!\n",
      "Model saved at epoch 22 with validation loss 3.3661!\n",
      "Model saved at epoch 23 with validation loss 3.2404!\n",
      "Model saved at epoch 24 with validation loss 3.1150!\n",
      "Model saved at epoch 25 with validation loss 2.9906!\n",
      "Model saved at epoch 26 with validation loss 2.8672!\n",
      "Model saved at epoch 27 with validation loss 2.7458!\n",
      "Model saved at epoch 28 with validation loss 2.6273!\n",
      "Model saved at epoch 29 with validation loss 2.5123!\n",
      "Model saved at epoch 30 with validation loss 2.3996!\n",
      "Model saved at epoch 31 with validation loss 2.2903!\n",
      "Model saved at epoch 32 with validation loss 2.1855!\n",
      "Model saved at epoch 33 with validation loss 2.0852!\n",
      "Model saved at epoch 34 with validation loss 1.9895!\n",
      "Model saved at epoch 35 with validation loss 1.8982!\n",
      "Model saved at epoch 36 with validation loss 1.8109!\n",
      "Model saved at epoch 37 with validation loss 1.7275!\n",
      "Model saved at epoch 38 with validation loss 1.6482!\n",
      "Model saved at epoch 39 with validation loss 1.5740!\n",
      "Model saved at epoch 40 with validation loss 1.5064!\n",
      "Model saved at epoch 41 with validation loss 1.4471!\n",
      "Model saved at epoch 42 with validation loss 1.4006!\n",
      "Model saved at epoch 43 with validation loss 1.3680!\n",
      "Model saved at epoch 44 with validation loss 1.3478!\n",
      "Model saved at epoch 45 with validation loss 1.3356!\n",
      "Model saved at epoch 46 with validation loss 1.3280!\n",
      "Model saved at epoch 47 with validation loss 1.3213!\n",
      "Model saved at epoch 48 with validation loss 1.3140!\n",
      "Model saved at epoch 49 with validation loss 1.3062!\n",
      "Model saved at epoch 50 with validation loss 1.2975!\n",
      "Model saved at epoch 51 with validation loss 1.2873!\n",
      "Model saved at epoch 52 with validation loss 1.2792!\n",
      "Model saved at epoch 53 with validation loss 1.2732!\n",
      "Model saved at epoch 54 with validation loss 1.2700!\n",
      "Model saved at epoch 55 with validation loss 1.2662!\n",
      "Model saved at epoch 56 with validation loss 1.2614!\n",
      "Model saved at epoch 57 with validation loss 1.2551!\n",
      "Model saved at epoch 58 with validation loss 1.2473!\n",
      "Model saved at epoch 59 with validation loss 1.2394!\n",
      "Model saved at epoch 60 with validation loss 1.2317!\n",
      "Model saved at epoch 61 with validation loss 1.2240!\n",
      "Model saved at epoch 62 with validation loss 1.2171!\n",
      "Model saved at epoch 63 with validation loss 1.2113!\n",
      "Model saved at epoch 64 with validation loss 1.2062!\n",
      "Model saved at epoch 65 with validation loss 1.2021!\n",
      "Model saved at epoch 66 with validation loss 1.1983!\n",
      "Model saved at epoch 67 with validation loss 1.1947!\n",
      "Model saved at epoch 68 with validation loss 1.1911!\n",
      "Model saved at epoch 69 with validation loss 1.1873!\n",
      "Model saved at epoch 70 with validation loss 1.1833!\n",
      "Model saved at epoch 71 with validation loss 1.1790!\n",
      "Model saved at epoch 72 with validation loss 1.1746!\n",
      "Model saved at epoch 73 with validation loss 1.1703!\n",
      "Model saved at epoch 74 with validation loss 1.1662!\n",
      "Model saved at epoch 75 with validation loss 1.1624!\n",
      "Model saved at epoch 76 with validation loss 1.1590!\n",
      "Model saved at epoch 77 with validation loss 1.1560!\n",
      "Model saved at epoch 78 with validation loss 1.1533!\n",
      "Model saved at epoch 79 with validation loss 1.1507!\n",
      "Model saved at epoch 80 with validation loss 1.1481!\n",
      "Model saved at epoch 81 with validation loss 1.1453!\n",
      "Model saved at epoch 82 with validation loss 1.1421!\n",
      "Model saved at epoch 83 with validation loss 1.1385!\n",
      "Model saved at epoch 84 with validation loss 1.1346!\n",
      "Model saved at epoch 85 with validation loss 1.1303!\n",
      "Model saved at epoch 86 with validation loss 1.1260!\n",
      "Model saved at epoch 87 with validation loss 1.1217!\n",
      "Model saved at epoch 88 with validation loss 1.1176!\n",
      "Model saved at epoch 89 with validation loss 1.1139!\n",
      "Model saved at epoch 90 with validation loss 1.1107!\n",
      "Model saved at epoch 91 with validation loss 1.1079!\n",
      "Model saved at epoch 92 with validation loss 1.1057!\n",
      "Model saved at epoch 93 with validation loss 1.1039!\n",
      "Model saved at epoch 94 with validation loss 1.1026!\n",
      "Model saved at epoch 95 with validation loss 1.1017!\n",
      "Model saved at epoch 96 with validation loss 1.1009!\n",
      "Model saved at epoch 97 with validation loss 1.1004!\n",
      "Model saved at epoch 98 with validation loss 1.1000!\n",
      "Model saved at epoch 99 with validation loss 1.0997!\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "wine = load_wine()\n",
    "X = torch.tensor(wine.data, dtype=torch.float32)\n",
    "y = torch.tensor(wine.target, dtype=torch.long)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Define the model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(13, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 3)\n",
    ")\n",
    "\n",
    "# Define criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Checkpointing parameters\n",
    "best_loss = float('inf')\n",
    "checkpoint_path = \"best_model.pth\"\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_valid)\n",
    "        val_loss = criterion(val_outputs, y_valid).item()\n",
    "    \n",
    "    # Save the model if the validation loss has decreased\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model, checkpoint_path)\n",
    "        print(f\"Model saved at epoch {epoch} with validation loss {val_loss:.4f}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ac60ab-adab-4199-8cef-de6b53bce34d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Model Training with Mini-Batches in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb60752-46e2-48e9-8bad-648dd5b2b25c",
   "metadata": {},
   "source": [
    "In machine learning, there are three main methods for training models: stochastic gradient descent (SGD), full-batch gradient descent, and mini-batch gradient descent. Let's explain these using a simple analogy.\n",
    "Imagine you are learning to shoot basketballs into a hoop:\n",
    "\n",
    "* **1. Stochastic Gradient Descent (SGD)**: This is like shooting one basketball, adjusting your aim after each shot. You get feedback quickly, but each shot can be influenced by random factors, making the learning process noisy.\n",
    "\n",
    "* **2. Full-Batch Gradient Descent**: This is like shooting all the basketballs you have, then reviewing your overall performance to adjust your aim. It gives you a clear picture but is slow and tiring because you have to shoot all the balls before making any adjustments.\n",
    "\n",
    "* **3. Mini-Batch Gradient Descent**: This method is a middle ground. It’s like shooting a few basketballs (say 10) before adjusting your aim. It’s faster than shooting all the balls at once and more stable than adjusting after every single shot, offering a balanced approach.\n",
    "\n",
    "Reasons to use it:\n",
    "\n",
    "* **1. Efficiency**: Processing smaller subsets of data significantly reduces memory usage and can take advantage of parallel processing hardware.\n",
    "  \n",
    "* **2. Convergence**: Provides a balance between noisy updates (SGD) and slow updates (full-batch), which can stabilize convergence.\n",
    "  \n",
    "* **3. Regularization**: Each mini-batch introduces some noise into the parameter updates, which can help prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce66ec6b-aef8-499d-be9d-ee1876f7b64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bathces: 6\n",
      "\n",
      "Batch Loss: 12.1249. Number of elements in this batch: 32\n",
      "Batch Loss: 12.2767. Number of elements in this batch: 32\n",
      "Batch Loss: 14.0084. Number of elements in this batch: 32\n",
      "Batch Loss: 11.6432. Number of elements in this batch: 32\n",
      "Batch Loss: 13.2737. Number of elements in this batch: 32\n",
      "Batch Loss: 12.9781. Number of elements in this batch: 18\n",
      "Epoch [1/10] Average Loss: 12.6970\n",
      "\n",
      "Batch Loss: 9.7494. Number of elements in this batch: 32\n",
      "Batch Loss: 7.2806. Number of elements in this batch: 32\n",
      "Batch Loss: 9.1786. Number of elements in this batch: 32\n",
      "Batch Loss: 9.4163. Number of elements in this batch: 32\n",
      "Batch Loss: 9.0569. Number of elements in this batch: 32\n",
      "Batch Loss: 9.9550. Number of elements in this batch: 18\n",
      "Epoch [2/10] Average Loss: 9.0394\n",
      "\n",
      "Batch Loss: 6.0275. Number of elements in this batch: 32\n",
      "Batch Loss: 7.7926. Number of elements in this batch: 32\n",
      "Batch Loss: 6.8153. Number of elements in this batch: 32\n",
      "Batch Loss: 4.5269. Number of elements in this batch: 32\n",
      "Batch Loss: 4.8252. Number of elements in this batch: 32\n",
      "Batch Loss: 5.1763. Number of elements in this batch: 18\n",
      "Epoch [3/10] Average Loss: 5.9145\n",
      "\n",
      "Batch Loss: 3.7812. Number of elements in this batch: 32\n",
      "Batch Loss: 3.9197. Number of elements in this batch: 32\n",
      "Batch Loss: 3.6029. Number of elements in this batch: 32\n",
      "Batch Loss: 2.9645. Number of elements in this batch: 32\n",
      "Batch Loss: 2.3045. Number of elements in this batch: 32\n",
      "Batch Loss: 1.8044. Number of elements in this batch: 18\n",
      "Epoch [4/10] Average Loss: 3.1619\n",
      "\n",
      "Batch Loss: 1.7129. Number of elements in this batch: 32\n",
      "Batch Loss: 1.5678. Number of elements in this batch: 32\n",
      "Batch Loss: 1.3272. Number of elements in this batch: 32\n",
      "Batch Loss: 1.0543. Number of elements in this batch: 32\n",
      "Batch Loss: 1.1015. Number of elements in this batch: 32\n",
      "Batch Loss: 1.2134. Number of elements in this batch: 18\n",
      "Epoch [5/10] Average Loss: 1.3386\n",
      "\n",
      "Batch Loss: 1.0949. Number of elements in this batch: 32\n",
      "Batch Loss: 1.1652. Number of elements in this batch: 32\n",
      "Batch Loss: 1.1882. Number of elements in this batch: 32\n",
      "Batch Loss: 1.1851. Number of elements in this batch: 32\n",
      "Batch Loss: 1.4606. Number of elements in this batch: 32\n",
      "Batch Loss: 1.3177. Number of elements in this batch: 18\n",
      "Epoch [6/10] Average Loss: 1.2288\n",
      "\n",
      "Batch Loss: 1.3021. Number of elements in this batch: 32\n",
      "Batch Loss: 1.2941. Number of elements in this batch: 32\n",
      "Batch Loss: 1.5635. Number of elements in this batch: 32\n",
      "Batch Loss: 1.4023. Number of elements in this batch: 32\n",
      "Batch Loss: 1.3361. Number of elements in this batch: 32\n",
      "Batch Loss: 1.5844. Number of elements in this batch: 18\n",
      "Epoch [7/10] Average Loss: 1.4003\n",
      "\n",
      "Batch Loss: 1.2937. Number of elements in this batch: 32\n",
      "Batch Loss: 1.4228. Number of elements in this batch: 32\n",
      "Batch Loss: 1.4055. Number of elements in this batch: 32\n",
      "Batch Loss: 1.5031. Number of elements in this batch: 32\n",
      "Batch Loss: 1.3151. Number of elements in this batch: 32\n",
      "Batch Loss: 1.2324. Number of elements in this batch: 18\n",
      "Epoch [8/10] Average Loss: 1.3723\n",
      "\n",
      "Batch Loss: 1.5808. Number of elements in this batch: 32\n",
      "Batch Loss: 1.1719. Number of elements in this batch: 32\n",
      "Batch Loss: 1.2709. Number of elements in this batch: 32\n",
      "Batch Loss: 1.1863. Number of elements in this batch: 32\n",
      "Batch Loss: 1.1423. Number of elements in this batch: 32\n",
      "Batch Loss: 1.1861. Number of elements in this batch: 18\n",
      "Epoch [9/10] Average Loss: 1.2619\n",
      "\n",
      "Batch Loss: 1.0957. Number of elements in this batch: 32\n",
      "Batch Loss: 0.9291. Number of elements in this batch: 32\n",
      "Batch Loss: 1.3555. Number of elements in this batch: 32\n",
      "Batch Loss: 1.0680. Number of elements in this batch: 32\n",
      "Batch Loss: 1.1956. Number of elements in this batch: 32\n",
      "Batch Loss: 1.0793. Number of elements in this batch: 18\n",
      "Epoch [10/10] Average Loss: 1.1238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "wine = load_wine()\n",
    "X = torch.tensor(wine.data, dtype=torch.float32)\n",
    "y = torch.tensor(wine.target, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for mini-batches\n",
    "batch_size = 32\n",
    "dataset = TensorDataset(X, y)\n",
    "data_loader = DataLoader(dataset, batch_size = batch_size, shuffle = True)   # data is shuffled at each epoch, improving the generalization capabilities of the model\n",
    "print(f\"Number of bathces: {len(data_loader)}\\n\")\n",
    "\n",
    "# Define the model \n",
    "model = nn.Sequential(\n",
    "    nn.Linear(13, 10),  \n",
    "    nn.ReLU(),  \n",
    "    nn.Linear(10, 10),  \n",
    "    nn.ReLU(),  \n",
    "    nn.Linear(10, 3)  \n",
    ")\n",
    "\n",
    "# Define criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "# Model training with mini-batches\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_X, batch_y in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Batch Loss: {loss.item():.4f}. Number of elements in this batch: {batch_X.shape[0]}')\n",
    "        running_loss += loss.item() * batch_X.size(0)   # this scaling is necessary because mini-batches can have different sizes, especially the last mini-batch, which might be smaller\n",
    "    \n",
    "    epoch_loss = running_loss / len(data_loader.dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] Average Loss: {epoch_loss:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a0b1a3-250d-4a5b-aeff-35d454ac6a79",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Learning Rate Scheduling in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376dada7-32dc-41a0-9986-6ea39e82bf75",
   "metadata": {},
   "source": [
    "Learning rate scheduling is a technique used to adjust the learning rate during training to improve model convergence and performance. We want to gradually reduce the LR during training to avoid overfitting.\n",
    "\n",
    "PyTorch offers several built-in learning rate schedulers to help manage the learning rate during training:\n",
    "\n",
    "* **StepLR**: Reduces the learning rate by a factor every few epochs.\n",
    "\n",
    "* **ExponentialLR**: Reduces the learning rate by a multiplicative factor.\n",
    "\n",
    "* **ReduceLROnPlateau**: Reduces the learning rate when a metric has stopped improving.\n",
    "\n",
    "Here we focus on the _ReduceLROnPlateau_ scheduler, which reduces the learning rate when a specified metric has stopped improving. This is useful in cases where the learning rate needs to adapt based on the performance of the model on a validation set, rather than following a predefined schedule.\n",
    "\n",
    "We initialize the _ReduceLROnPlateau scheduler_ with a _mode_ of 'min', a _factor_ value of 0.1, and a _patience_ of 10 epochs. The mode = 'min' setting is used when you want to reduce the learning rate as soon as the monitored quantity (_validation loss_) stops decreasing. Essentially, if the validation loss does not improve (i.e., reduces) for 10 consecutive epochs, the scheduler will decrease the learning rate by a factor of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "273df2d6-d97a-4f93-b201-e3390b92cfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], loss_test: 1.08774710 LR: 0.100000\n",
      "Epoch [20/100], loss_test: 1.08734059 LR: 0.100000\n",
      "Epoch [30/100], loss_test: 1.08659446 LR: 0.010000\n",
      "Epoch [40/100], loss_test: 1.08499372 LR: 0.001000\n",
      "Epoch [50/100], loss_test: 1.08491313 LR: 0.000100\n",
      "Epoch [60/100], loss_test: 1.08490956 LR: 0.000010\n",
      "Epoch [70/100], loss_test: 1.08490932 LR: 0.000001\n",
      "Epoch [80/100], loss_test: 1.08490908 LR: 0.000000\n",
      "Epoch [90/100], loss_test: 1.08490908 LR: 0.000000\n",
      "Epoch [100/100], loss_test: 1.08490908 LR: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "wine = load_wine()\n",
    "X = torch.tensor(wine.data, dtype=torch.float32)\n",
    "y = torch.tensor(wine.target, dtype=torch.long)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(13, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 3)\n",
    ")\n",
    "\n",
    "# Define criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.1)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.1, patience = 10)\n",
    "\n",
    "# Model training with learning rate scheduling\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_valid)\n",
    "        val_loss = criterion(val_outputs, y_valid)\n",
    "    \n",
    "    scheduler.step(val_loss)  # Update learning rate based on validation loss\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], loss_test: {val_loss:.8f} LR: {lr:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7990a182-384f-4fca-be1a-78421ef30eb7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Overfitting Prevention with Regularization and Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6103e52-b3ab-4c0b-b089-76c50a24c208",
   "metadata": {},
   "source": [
    "**Regularization** is a technique used to prevent overfitting in machine learning models. Overfitting occurs when a model learns not only the underlying patterns in the training data but also the random noise, leading to poor performance on new, unseen data. Regularization techniques add additional constraints or penalties to the model's learning process, encouraging it to focus on the most important patterns in the data and improving its generalization ability.\n",
    "\n",
    "* **Dropout** is a regularization technique used to prevent overfitting in neural networks. During training, dropout layers randomly set a fraction of activations in the preceding layer to zero at each update. This prevents the network from becoming too dependent on any particular neurons, thereby promoting generalization.\n",
    "The fraction of activations set to zero is determined by the dropout rate, typically a value between 0 and 1. For instance, a dropout rate of 0.2 implies that 20% of the neurons will be randomly dropped during each iteration of training. This technique helps in creating a more robust model that performs better on unseen data.\n",
    "Dropout layers are added with _nn.Dropout(fraction)_\n",
    "\n",
    "* **L2 regularization** is another regularization technique which penalizes large weights by adding their squared values to the loss function. This discourages the model from relying too heavily on any particular feature, thereby enhancing generalization. In PyTorch, L2 regularization is typically implemented by specifying the _weight_decay_ parameter in the optimizer.\n",
    "We start training a model without L2 regularization and introduce it midway to observe its effect. We are also going to print the L2 norm of the weights in the first layer at different epochs to observe how they evolve over time. The L2 norm is a measure of the magnitude of the weights, calculated as the square root of the sum of the squared weights. It gives an indication of how large the weights are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3d2cf997-0c5f-45c6-9b09-04e2e20965c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=13, out_features=10, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.2, inplace=False)\n",
      "  (3): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.1, inplace=False)\n",
      "  (6): Linear(in_features=10, out_features=3, bias=True)\n",
      ")\n",
      "\n",
      "10 - L2 norm of weights: 1.7177019119262695\n",
      "20 - L2 norm of weights: 1.720146894454956\n",
      "30 - L2 norm of weights: 1.7237865924835205\n",
      "40 - L2 norm of weights: 1.7300032377243042\n",
      "50 - L2 norm of weights: 1.7370929718017578\n",
      "\n",
      "Regularization added to optimizer\n",
      "\n",
      "60 - L2 norm of weights: 1.6976491212844849\n",
      "70 - L2 norm of weights: 1.6549012660980225\n",
      "80 - L2 norm of weights: 1.613250970840454\n",
      "90 - L2 norm of weights: 1.5738153457641602\n",
      "100 - L2 norm of weights: 1.53805673122406\n"
     ]
    }
   ],
   "source": [
    "# Load and select the data\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Define the model with dropout\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(13, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),  # Dropout applied to the previous layer\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),  # Dropout applied to the previous layer\n",
    "    nn.Linear(10, 3)\n",
    ")\n",
    "\n",
    "# Print the model summary\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "# Defining criterion and optimizer without weight decay\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  \n",
    "\n",
    "for i in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()  # Zero the gradient buffers\n",
    "    outputs = model(X)  # Forward pass\n",
    "    loss = criterion(outputs, y)  # Compute loss\n",
    "    loss.backward()  # Backward pass\n",
    "\n",
    "    if(i==50):\n",
    "        # Introducing weight decay from 50th epoch on\n",
    "        optimizer = optim.Adam(model.parameters(), lr = 0.001, weight_decay = 0.01) \n",
    "        print(\"\\nRegularization added to optimizer\\n\")\n",
    "\n",
    "    if (i+1) % 10 ==0:    \n",
    "        # L2 norm of weights of the first linear layer \n",
    "        first_layer_weights = model[0].weight.norm(2).item()\n",
    "        print(f'{i+1} - L2 norm of weights: {first_layer_weights}')\n",
    "\n",
    "    optimizer.step()  # Update weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
